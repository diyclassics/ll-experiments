{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Diction and Topics in Latin Love Elegy\n",
    "***With the Classical Language Toolkit***\n",
    "\n",
    "Patrick J. Burns\n",
    "Institute for the Study of the Ancient World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow along at: https://github.com/diyclassics/ll-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary Python packages\n",
    "- pip install jupyter\n",
    "- pip install numpy pandas matplotlib sklearn\n",
    "- pip install cltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An iPython notebook to demonstrate the use of the Classical Language Toolkit for developing a natural language processing workflow for Latin texts and using this workflow for the basis of exploratory data analysis of diction and topics in these texts. Based on Allen Riddell's [»Text Analysis with Topic Models for the Humanities and Social Sciences«](https://de.dariah.eu/tatom/index.html).\n",
    "\n",
    "Presented at:\n",
    "UT Austin Classics, 11/30/18\n",
    "ISAW Intro to Digital Humanities course, 12/4/17  \n",
    "Yale University Classics Department, 5/18/17  \n",
    "  \n",
    "Last updated 11/27/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up corpora\n",
    "\n",
    "## You will need the models/datasets that the new lemmatizer uses\n",
    "## Note that this will generate an error if this and older \n",
    "## version of this corpus is already installed. If that happens,\n",
    "## backup the old version, move or delete it, and reimport the corpus.\n",
    "\n",
    "#from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "#corpus_importer = CorpusImporter('latin')\n",
    "#corpus_importer.list_corpora\n",
    "\n",
    "#corpus_importer.import_corpus('latin_models_cltk')\n",
    "\n",
    "## We will be using the Latin Library corpus for today's workshop,\n",
    "## so we will also need to import that as well.\n",
    "\n",
    "#corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, you want to keep imports at the front of a project, but for\n",
    "# the purposes of today's workshop, it seemed better to introduce them on\n",
    "# an as-needed basis. Still, here is an example of what I would usually do\n",
    "# at the beginning of a notebook.\n",
    "\n",
    "from pprint import pprint \n",
    "\n",
    "# pprint allows for cleaner presentation of Python objects, esp. long strings,\n",
    "# lists (and nested lists), dictionaries, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Latin Library corpus\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "\n",
    "# The CLTK Latin Library corpus is a web-scraped collection of plaintext files\n",
    "# from thelatinlibrary.com.\n",
    "\n",
    "# We can access the individual files as follows...\n",
    "\n",
    "files = latinlibrary.fileids()\n",
    "print(files[:50]) # The first 50 files in the corpus\n",
    "\n",
    "# Note the [:50] slice to limit our list to the first 50 items.\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can iterate over the list of files to retrieve specific files. Today\n",
    "# we want to work with book 1 of Propertius and book 1 of Tibullus. We\n",
    "# can isolate these files by using:\n",
    "# 1. list comprehension\n",
    "# 2. testing for membership with 'in'\n",
    "\n",
    "# NB: Python designates lists with square brackets, i.e. [...]; list comprehensions\n",
    "# build lists in place with the following syntax...\n",
    "\n",
    "[file for file in files if 'prop' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are all the files in the Latin Library with 'prop' in the filename. We\n",
    "# are looking for 'propertius1.txt' and can assign it to a variable as follows...\n",
    "\n",
    "propertius_file = 'propertius1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for Tibullus\n",
    "\n",
    "print([file for file in files if 'tib' in file])\n",
    "print('\\n')\n",
    "pprint([file for file in files if 'tib' in file])\n",
    "\n",
    "tibullus_file = 'tibullus1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use raw function for plaintextcorpus reader to get the contents of\n",
    "# the file as a string...\n",
    "\n",
    "propertius_raw = latinlibrary.raw(propertius_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a preview of the contents...\n",
    "\n",
    "print(propertius_raw[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a bit of the end...\n",
    "\n",
    "print(propertius_raw[-500:]) # Note the negative slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Tibullus texts\n",
    "\n",
    "tibullus_raw = latinlibrary.raw(tibullus_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for preprocessing\n",
    "\n",
    "import re # Regex module, useful for pattern matching\n",
    "import html # Useful for handling entities\n",
    "\n",
    "# Import/load a CLTK tool for normalizing i/j and u/v in Latin texts\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "replacer = JVReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "# I've written the following preprocessing function for today's workshop. It's\n",
    "# pretty specific in a few cases—more so than I would normally would use, e.g. '8a'\n",
    "# and '8b'.\n",
    "\n",
    "# Still the goal of preprocessing is to make all of the texts we are working with\n",
    "# for this experiment as uniform as possible, so that functions/processes further along\n",
    "# encounter consistent data. GIGO, as they say. E.g., for today we want\n",
    "# 'verbum' and 'uerbum' to be treated as the same word. For a different experiment, \n",
    "# you might not and need to make different editorial choices.\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Remove Latin Library-specific paratexts with regex\n",
    "    \n",
    "    remove_list = [r'\\bThe Latin Library\\b',\n",
    "                   r'\\bThe Classics Page\\b',\n",
    "                   r'\\bPropertius\\b',\n",
    "                   r'\\bSEXTI PROPERTI ELEGIARVM LIBER PRIMVS\\b',\n",
    "                   r'8a', r'8b', r'VIIIA', r'VIIIB',\n",
    "                   r'\\bTibullus Book I\\b',\n",
    "                   r'\\bTibullus',\n",
    "                   r'\\bA. TIBVLLI ELEGIAE LIBER PRIMVS\\b',\n",
    "                   r'I II III IV V VI VII VIII IX X',\n",
    "                   r'25a', r'25b'\n",
    "                  ]\n",
    "    \n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remove html entities and related html artifacts\n",
    "    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "    text = re.sub(r'\\x00',' ',text) #Another space problem?\n",
    "    text = re.sub(r' \\xa0 ', '    ', text)\n",
    "    \n",
    "    # Remove roman numeral headings; must be before lower & replacer\n",
    "    text = re.sub(r'\\b(M{1,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})|M{0,4}(CM|C?D|D?C{1,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,4})|M{0,4}(CM|CD|D?C{0,3})(XC|X?L|L?X{1,3})(IX|IV|V?I{0,3})|M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|I?V|V?I{1,3}))\\b',' ',text)\n",
    "    \n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize text\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    # Remove punctuation with translate\n",
    "    punctuation =\"\\\"#$%&\\'()+,-/:;<=>@[\\]^_`{|}~.?!«»—\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Remove numbers\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Handle spacing\n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\t',' ', text) # Remove tabs\n",
    "    text = re.sub('^\\s+','', text)\n",
    "    text = re.sub(' \\n', '\\n', text)\n",
    "    text = re.sub('\\n\\n', '~', text)\n",
    "    text = re.sub('~+', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "propertius_edit = preprocess(propertius_raw)\n",
    "tibullus_edit = preprocess(tibullus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview text\n",
    "\n",
    "print(propertius_edit[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to raw plaintext from LL...\n",
    "\n",
    "print(propertius_raw[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into list of poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have set up the preprocessing to leave two blank lines between\n",
    "# each poem in the original string. We can use 'split' to break the\n",
    "# text up into a list of strings, each string containing one poem.\n",
    "\n",
    "propertius = propertius = propertius_edit.split('\\n\\n')\n",
    "tibullus = tibullus_edit.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length of list\n",
    "\n",
    "print(len(propertius))\n",
    "print(len(tibullus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first line of each poem in Propertius list...\n",
    "\n",
    "for poem in propertius:\n",
    "    print(poem[:poem.find('\\n')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tibullus_titles = ['Tib. 1.1', 'Tib. 1.2', 'Tib. 1.3', 'Tib. 1.4', 'Tib. 1.5', \n",
    "                   'Tib. 1.6', 'Tib. 1.7', 'Tib. 1.8', 'Tib. 1.9', 'Tib. 1.10']\n",
    "\n",
    "propertius_titles = ['Prop. 1.1', 'Prop. 1.2', 'Prop. 1.3', 'Prop. 1.4', 'Prop. 1.5', \n",
    "                     'Prop. 1.6', 'Prop. 1.7', 'Prop. 1.8a', 'Prop. 1.8b', 'Prop. 1.9', \n",
    "                     'Prop. 1.10', 'Prop. 1.11', 'Prop. 1.12', 'Prop. 1.13', 'Prop. 1.14', \n",
    "                     'Prop. 1.15', 'Prop. 1.16', 'Prop. 1.17', 'Prop. 1.18', 'Prop. 1.19', \n",
    "                     'Prop. 1.20', 'Prop. 1.21', 'Prop. 1.22']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview titles with first lines…\n",
    "\n",
    "for i, poem in enumerate(propertius[:5]):\n",
    "    print(propertius_titles[i],\n",
    "          '\\n',\n",
    "          poem[:poem.find('\\n')],\n",
    "          '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is what we have...\n",
    "\n",
    "print(propertius[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we want is a list of all of the words in the poem. This is word tokenization and\n",
    "# it will largely be done by splitting the text on whitespace, thought the CLTK\n",
    "# has language-specific tokenization to handle language-specific problems, e.g.\n",
    "# What would you want to extract from 'arma virumque cano'? ['virum'] or ['virum', '-que']?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CLTK Latin word tokenizer\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "word_tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is what the same poem looks like as a list of tokens\n",
    "print(word_tokenizer.tokenize(propertius[0]))\n",
    "print(len(word_tokenizer.tokenize(propertius[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize list of poems\n",
    "\n",
    "# Again we can use list comprehensions to create orderly lists of lists of tokens.\n",
    "\n",
    "propertius_tokens = [word_tokenizer.tokenize(poem) for poem in propertius]\n",
    "tibullus_tokens = [word_tokenizer.tokenize(poem) for poem in tibullus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we now want is a list of all of the lemmata, i.e. the dictionary headword, in \n",
    "# the poem. CLTK has Latin-specific lemmatizers that make automatic and 'informed' \n",
    "# decisions about which headword to pick. The accuracy of the lemmatizer we will\n",
    "# use today—the backoff lemmatizer—is constantly improving and is roughly 90% at present,\n",
    "# which should be sufficient for today's experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import a data model to train the lemmatizer.\n",
    "\n",
    "import os\n",
    "from cltk.utils.file_operations import open_pickle\n",
    "\n",
    "# Set up training sentences\n",
    "\n",
    "rel_path = os.path.join('~/cltk_data/latin/model/latin_models_cltk/lemmata/backoff')\n",
    "path = os.path.expanduser(rel_path)\n",
    "\n",
    "# Check for presence of latin_pos_lemmatized_sents\n",
    "file = 'latin_pos_lemmatized_sents.pickle'      \n",
    "\n",
    "latin_pos_lemmatized_sents_path = os.path.join(path, file)\n",
    "if os.path.isfile(latin_pos_lemmatized_sents_path):\n",
    "    latin_pos_lemmatized_sents = open_pickle(latin_pos_lemmatized_sents_path)\n",
    "else:\n",
    "    latin_pos_lemmatized_sents = []\n",
    "    print('The file %s is not available in cltk_data' % file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CLTK Latin backoff lemmatizer\n",
    "\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "lemmatizer = BackoffLatinLemmatizer(latin_pos_lemmatized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lemmatizer, while more accurate than others, can be **very** slow. So, for\n",
    "# the purposes of today's workshop, we will use a 'trick' to speed it up, i.e.\n",
    "# rejoin the list of tokens and process two strings rather than a list of all the tokens.\n",
    "\n",
    "## Make one string of all files, delimited by |||\n",
    "## (NB: Attempt to reduce the number of regex compiles.)\n",
    "\n",
    "def create_lemmatized_texts(texts):\n",
    "    textin = \" ||| \".join(texts)\n",
    "    tokens = textin.split()\n",
    "    lemmas = lemmatizer.lemmatize(tokens)\n",
    "    textout = \" \".join(lemma[1] for lemma in lemmas)\n",
    "    punctuation =\"\\\"#$%&\\'()+,-/:;<=>@[\\]^_`{}~.?!«»—\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    textout = textout.translate(translator)\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    textout = textout.translate(translator)\n",
    "    textout = re.sub(r' punc ', ' ', textout)\n",
    "    lemmatized_texts = textout.split('|||')\n",
    "    return lemmatized_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_lemmatized_texts(propertius[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As noted above, rejoin list of tokens...\n",
    "propertius_tokenized_texts = [' '.join(tokens) for tokens in propertius_tokens]\n",
    "tibullus_tokenized_texts = [' '.join(tokens) for tokens in tibullus_tokens]\n",
    "\n",
    "# ... and lemmatize.\n",
    "propertius_lemmatized_texts = create_lemmatized_texts(propertius_tokenized_texts)\n",
    "tibullus_lemmatized_texts = create_lemmatized_texts(tibullus_tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's split our lemmatized texts into a list of lemmas, first by getting\n",
    "# a list of lemmas per poem...\n",
    "\n",
    "propertius_lemmas = [text.split() for text in propertius_lemmatized_texts]\n",
    "tibullus_lemmas = [text.split() for text in tibullus_lemmatized_texts]\n",
    "\n",
    "# ... then by flattening these lists into a single author list.\n",
    "\n",
    "propertius_lemmas = [item for sublist in propertius_lemmas for item in sublist]\n",
    "tibullus_lemmas = [item for sublist in tibullus_lemmas for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a list of lemmas from our authors, we can use the Counter\n",
    "# object in Python to get some frequency information about \"words\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Build counter of top token counts\n",
    "propertius_lemmas_counter = Counter(propertius_lemmas)\n",
    "propertius_lemmas_mc = propertius_lemmas_counter.most_common(100)\n",
    "\n",
    "running = 0\n",
    "\n",
    "print('Top 25 lemmas in Propertius 1:\\n')\n",
    "print(\"{number:>5}  {lemma:<12}{count:<12}{percent:<12}{running:<12}\".format(number=\"\", lemma=\"lemma\", count=\"COUNT\", percent=\"Type-Tok %\", running = \"RUNNING %\"))\n",
    "for i, pair in enumerate(propertius_lemmas_mc[:25]):\n",
    "    running += pair[1]\n",
    "    print(\"{number:>5}. {lemma:<12}{count:<12}{percent:<12}{running:<12}\".format(number=i+1, lemma=pair[0], count=pair[1], percent=str(round(pair[1] / len(propertius_lemmas)*100, 2))+\"%\", running = str(round(running / len(propertius_lemmas)*100, 2))+\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build counter of top token counts\n",
    "tibullus_lemmas_counter = Counter(tibullus_lemmas)\n",
    "tibullus_lemmas_mc = tibullus_lemmas_counter.most_common(100)\n",
    "\n",
    "running = 0\n",
    "\n",
    "print('Top 25 lemmas in Tibullus 1:\\n')\n",
    "print(\"{number:>5}  {lemma:<12}{count:<12}{percent:<12}{running:<12}\".format(number=\"\", lemma=\"lemma\", count=\"COUNT\", percent=\"Type-Tok %\", running = \"RUNNING %\"))\n",
    "for i, pair in enumerate(tibullus_lemmas_mc[:25]):\n",
    "    running += pair[1]\n",
    "    print(\"{number:>5}. {lemma:<12}{count:<12}{percent:<12}{running:<12}\".format(number=i+1, lemma=pair[0], count=pair[1], percent=str(round(pair[1] / len(tibullus_lemmas)*100, 2))+\"%\", running = str(round(running / len(tibullus_lemmas)*100, 2))+\"%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will quickly go through the kinds of exploratory data analysis available\n",
    "# to us once we have preprocessed or tokenized or lemmatized texts, spec.\n",
    "\n",
    "# - Visualizing texts by \"distance\" based on vocabulary\n",
    "# - Visualizing automatically extracted topics based on word co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "# Create an instance of a count vectorizer\n",
    "vectorizer = text.CountVectorizer(input='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-term matrix, i.e. reduce our texts to an array where\n",
    "# - the horizontal axis is a list of words\n",
    "# - the vertical axis is a list of documents\n",
    "# - the intersection of the two axes is the count of the word in the document\n",
    "\n",
    "# Take two sentences:\n",
    "# - Omnia vincit amor.\n",
    "# - Omnia vincit labor.\n",
    "\n",
    "# Reduce the setences to document-term matrix\n",
    "dtm = vectorizer.fit_transform(['Omnia vincit amor.', 'Omnia vincit labor.'])\n",
    "dtm = dtm.toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "# Print the complete list of words from both sentences\n",
    "print(vocab)\n",
    "print('\\n')\n",
    "\n",
    "# Print a table with the dtm\n",
    "pd.DataFrame(dtm, columns=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously for a complete work, the matrix is much larger (and more\n",
    "# sparse, i.e. lots of zeroes).\n",
    "\n",
    "dtm = vectorizer.fit_transform(propertius_lemmatized_texts)\n",
    "dtm = dtm.toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "# Print the complete list of words from both sentences\n",
    "print(vocab)\n",
    "print('\\n')\n",
    "\n",
    "# Print a table with the dtm\n",
    "pd.DataFrame(dtm, columns=vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Euclidean distance across the document-term matrix\n",
    "dist = 1 - cosine_similarity(dtm)\n",
    "\n",
    "# Reduce dimensionality\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "\n",
    "# Get coordinates\n",
    "pos = mds.fit_transform(dist)\n",
    "xs, ys = pos[:,0], pos[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatterplot\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Euclidean Distance for Vocabulary in Propertius 1')\n",
    "plt.scatter(xs, ys, cmap='spring')\n",
    "for x, y, name in zip(xs, ys, propertius_titles):\n",
    "    plt.text(x,y,name,alpha=0.5,fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view these distance measures as a branching tree as well\n",
    "\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('\"Dendrogram\" Distance for Vocabulary in Propertius 1')\n",
    "dendrogram(linkage_matrix, orientation=\"left\", labels=propertius_titles, leaf_font_size=12);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can combine our two elegists to visualize them together\n",
    "\n",
    "texts = propertius_lemmatized_texts + tibullus_lemmatized_texts\n",
    "titles = propertius_titles + tibullus_titles\n",
    "\n",
    "dtm = vectorizer.fit_transform(texts)\n",
    "dtm = dtm.toarray()\n",
    "\n",
    "dist = 1 - cosine_similarity(dtm)\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "pos = mds.fit_transform(dist)\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('\"Dendrogram\" Distance for Vocabulary in Propertius 1 & Tibullus 1')\n",
    "dendrogram(linkage_matrix, orientation=\"left\", labels=titles, leaf_font_size=12);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"The fact that four words (out of eighty-four) in a Propertian passage, and words that are hardly unusual or remarkable, bear some similarity to four words (out ofthirty-four) in some lines of Tibullus provides extremely weak grounds for claiming Propertian influence.\" (Murgatroyd, P. 1982. \"Reply to Review,\" *Acta Classica* 25: 145-147.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Latin stopword list\n",
    "\n",
    "from cltk.stop.latin.stops import STOPS_LIST as latin_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_stops = latin_stops[:91]\n",
    "latin_stops.extend(['que'])\n",
    "print(latin_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "vectorizer = text.CountVectorizer(input='content', stop_words=latin_stops, min_df=2) # Note cutoff of two words\n",
    "dtm = vectorizer.fit_transform(texts).toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "num_topics = 10\n",
    "num_top_words = 25\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1) # Using non-negative matrix factorization\n",
    "\n",
    "# For more on NMF, see https://de.dariah.eu/tatom/topic_model_python.html\n",
    "\n",
    "doctopic = clf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of words per \"topic\" (i.e. classifier component)\n",
    "\n",
    "topic_words = []\n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words.append([vocab[i] for i in word_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the document-component matrix such that the component values\n",
    "# associated with each document sum to one.\n",
    "\n",
    "doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = np.asarray(titles)\n",
    "doctopic_orig = doctopic.copy()\n",
    "\n",
    "# Create empty matrix\n",
    "num_groups = len(set(names))\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate matrix \n",
    "\n",
    "for i, name in enumerate(sorted(set(names))):\n",
    "    doctopic_grouped[i, :] = np.mean(doctopic[names == name, :], axis=0)\n",
    "\n",
    "doctopic = doctopic_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe for clearer display\n",
    "\n",
    "df = pd.DataFrame(data=doctopic, index=titles).round(2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top three topics per document\n",
    "\n",
    "print(\"Top NMF topics in...\")\n",
    "for i in range(len(doctopic)):\n",
    "    top_topics = np.argsort(doctopic[i,:])[::-1][0:3]\n",
    "    top_topics_str = ' '.join(str(t) for t in top_topics)\n",
    "    print(\"{}: {}\".format(titles[i], top_topics_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show words associated with each document\n",
    "\n",
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:25])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make heatmap of topics\n",
    "\n",
    "N, K = doctopic.shape  # N documents, K topics\n",
    "topic_labels = ['Topic #{}'.format(k) for k in range(K)]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.title(\"Heatmap of Topics in Propertius 1 & Tibullus 1\")\n",
    "plt.pcolor(doctopic, norm=None, cmap='Reds')\n",
    "plt.yticks(np.arange(doctopic.shape[0])+0.5, names);\n",
    "plt.xticks(np.arange(doctopic.shape[1])+0.5, topic_labels);\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(rotation=90)\n",
    "plt.colorbar(cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show list of words again\n",
    "\n",
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:15])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_stops.extend(['cynthia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "vectorizer = text.CountVectorizer(input='content', stop_words=latin_stops, min_df=2) # Note cutoff of two words\n",
    "dtm = vectorizer.fit_transform(texts).toarray()\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "num_topics = 10\n",
    "num_top_words = 25\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1) # Using non-negative matrix factorization\n",
    "\n",
    "# For more on NMF, see https://de.dariah.eu/tatom/topic_model_python.html\n",
    "\n",
    "doctopic = clf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of words per \"topic\" (i.e. classifier component)\n",
    "\n",
    "topic_words = []\n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the document-component matrix such that the component values\n",
    "# associated with each document sum to one.\n",
    "\n",
    "doctopic = doctopic / np.sum(doctopic, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = np.asarray(titles)\n",
    "doctopic_orig = doctopic.copy()\n",
    "\n",
    "# Create empty matrix\n",
    "num_groups = len(set(names))\n",
    "doctopic_grouped = np.zeros((num_groups, num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate matrix \n",
    "\n",
    "for i, name in enumerate(sorted(set(names))):\n",
    "    doctopic_grouped[i, :] = np.mean(doctopic[names == name, :], axis=0)\n",
    "\n",
    "doctopic = doctopic_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe for clearer display\n",
    "\n",
    "df = pd.DataFrame(data=doctopic, index=titles).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show words associated with each document\n",
    "\n",
    "for t in range(len(topic_words)):\n",
    "    print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:25])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make heatmap of topics\n",
    "\n",
    "N, K = doctopic.shape  # N documents, K topics\n",
    "topic_labels = ['Topic #{}'.format(k) for k in range(K)]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.title(\"Heatmap of Topics in Propertius 1 & Tibullus 1 (without 'cynthia')\")\n",
    "plt.pcolor(doctopic, norm=None, cmap='Reds')\n",
    "plt.yticks(np.arange(doctopic.shape[0])+0.5, names);\n",
    "plt.xticks(np.arange(doctopic.shape[1])+0.5, topic_labels);\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(rotation=90)\n",
    "plt.colorbar(cmap='Reds')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
