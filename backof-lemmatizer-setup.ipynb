{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install CLTK Latin Models\n",
    "\n",
    "# # Commented out, because you only need to run it once to download\n",
    "# # the model to the local machine\n",
    "\n",
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "# corpus_importer = CorpusImporter('latin')\n",
    "# corpus_importer.list_corpora\n",
    "# corpus_importer.import_corpus('latin_models_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a data model to train the lemmatizer\n",
    "\n",
    "import os\n",
    "from cltk.utils.file_operations import open_pickle\n",
    "\n",
    "# Set up training sentences\n",
    "\n",
    "rel_path = os.path.join('~/cltk_data/latin/model/latin_models_cltk/lemmata/backoff')\n",
    "path = os.path.expanduser(rel_path)\n",
    "\n",
    "# Check for presence of latin_pos_lemmatized_sents\n",
    "file = 'latin_pos_lemmatized_sents.pickle'      \n",
    "\n",
    "latin_pos_lemmatized_sents_path = os.path.join(path, file)\n",
    "if os.path.isfile(latin_pos_lemmatized_sents_path):\n",
    "    latin_pos_lemmatized_sents = open_pickle(latin_pos_lemmatized_sents_path)\n",
    "else:\n",
    "    latin_pos_lemmatized_sents = []\n",
    "    print('The file %s is not available in cltk_data' % file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CLTK Latin backoff lemmatizer\n",
    "\n",
    "# Prebuilt backoff chainâ€”let me know if you want more information\n",
    "# on building you own chain.\n",
    "\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "lemmatizer = BackoffLatinLemmatizer(latin_pos_lemmatized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CLTK Latin word tokenizer\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample text\n",
    "\n",
    "cat1 = 'Quo usque tandem abutere, Catilina, patientia nostra? quam diu etiam furor iste tuus nos eludet? quem ad finem sese effrenata iactabit audacia? Nihilne te nocturnum praesidium Palati, nihil urbis vigiliae, nihil timor populi, nihil concursus bonorum omnium, nihil hic munitissimus habendi senatus locus, nihil horum ora voltusque moverunt? Patere tua consilia non sentis, constrictam iam horum omnium scientia teneri coniurationem tuam non vides? Quid proxima, quid superiore nocte egeris, ubi fueris, quos convocaveris, quid consilii ceperis, quem nostrum ignorare arbitraris? O tempora, o mores! Senatus haec intellegit. Consul videt; hic tamen vivit. Vivit? immo vero etiam in senatum venit, fit publici consilii particeps, notat et designat oculis ad caedem unum quemque nostrum. Nos autem fortes viri satis facere rei publicae videmur, si istius furorem ac tela vitemus. Ad mortem te, Catilina, duci iussu consulis iam pridem oportebat, in te conferri pestem, quam tu in nos [omnes iam diu] machinaris.'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quo', 'usque', 'tandem', 'abutere', ',', 'catilina', ',', 'patientia', 'nostra', '?']\n"
     ]
    }
   ],
   "source": [
    "# Get tokens\n",
    "\n",
    "tokens = tokenizer.tokenize(cat1)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('quo', 'quo'), ('usque', 'usque'), ('tandem', 'tandem'), ('abutere', 'abutor'), (',', 'punc'), ('catilina', 'catilina'), (',', 'punc'), ('patientia', 'patientia'), ('nostra', 'noster'), ('?', 'punc')]\n"
     ]
    }
   ],
   "source": [
    "# Get lemmas\n",
    "\n",
    "lemmas = lemmatizer.lemmatize(tokens)\n",
    "print(lemmas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lemmatizer to pickle for quick setup\n",
    "\n",
    "import pickle\n",
    "pickle.dump(lemmatizer, open(\"backoff.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load lemmatizer from pickle\n",
    "\n",
    "# import pickle\n",
    "# lemmatizer = pickle.load(open('backoff.p', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
